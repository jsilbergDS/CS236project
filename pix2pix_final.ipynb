{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pix2pix-final",
      "provenance": [],
      "collapsed_sections": []
    },
    "environment": {
      "name": "tf2-gpu.2-3.m74",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from: https://github.com/phillipi/pix2pix"
      ],
      "metadata": {
        "id": "77WnZ8TaXtd0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRm-USlsHgEV",
        "outputId": "c8b1fb6c-2658-4747-f9ea-cdc1c650e6cf"
      },
      "source": [
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2443, done.\u001b[K\n",
            "remote: Total 2443 (delta 0), reused 0 (delta 0), pack-reused 2443\u001b[K\n",
            "Receiving objects: 100% (2443/2443), 8.13 MiB | 28.13 MiB/s, done.\n",
            "Resolving deltas: 100% (1570/1570), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt3igws3eiVp"
      },
      "source": [
        "import os\n",
        "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1EySlOXwwoa",
        "outputId": "64750062-575a-4a06-f079-016903300eee"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.11.1+cu111)\n",
            "Collecting dominate>=2.4.0\n",
            "  Downloading dominate-2.6.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting visdom>=0.1.8.8\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[K     |████████████████████████████████| 676 kB 19.9 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (22.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.15.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (2.8.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (7.1.2)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.13)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.0-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 69.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (2.3)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.17.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb->-r requirements.txt (line 5)) (1.1.0)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Building wheels for collected packages: visdom, subprocess32, pathtools, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=f546c69af7f5c4ca7efdfb4ca92f43c2df3a960eb1f1bdcaef438d4760f9f414\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=a38e07861d5a140017f5dfd507ac8a1c2f6fbbe6b740945b331526725cdea5dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=04f9bda4c1c613ac2af815eceb56b4b1231e5766108cc94a272caab86e92da73\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5710 sha256=9b5a630c40380cc753efce52772c15f0ac5493a4699ac66183f3f55d023d9087\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built visdom subprocess32 pathtools torchfile\n",
            "Installing collected packages: smmap, jsonpointer, gitdb, yaspin, websocket-client, torchfile, subprocess32, shortuuid, sentry-sdk, pathtools, jsonpatch, GitPython, docker-pycreds, configparser, wandb, visdom, dominate\n",
            "Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 dominate-2.6.0 gitdb-4.0.9 jsonpatch-1.32 jsonpointer-2.2 pathtools-0.1.2 sentry-sdk-1.5.0 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 torchfile-0.1.0 visdom-0.1.8.9 wandb-0.12.7 websocket-client-1.2.1 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I pair the original highway images with my hand-drawn masks"
      ],
      "metadata": {
        "id": "dhfo6krnXxSk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3ZW6ddI5_W9",
        "outputId": "cea45581-84e7-4b12-af93-663e3ee8374e"
      },
      "source": [
        "!python datasets/combine_A_and_B.py --fold_A /content/drive/MyDrive/BayAreaHighway250Raw_pix --fold_B /content/drive/MyDrive/BayAreaHighway250Mask2 --fold_AB /content/drive/MyDrive/BayAreaHighway250AB_pix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold_A] =  /content/drive/MyDrive/BayAreaHighway250Raw_pix\n",
            "[fold_B] =  /content/drive/MyDrive/BayAreaHighway250Mask2\n",
            "[fold_AB] =  /content/drive/MyDrive/BayAreaHighway250AB_pix\n",
            "[num_imgs] =  1000000\n",
            "[use_AB] =  False\n",
            "[no_multiprocessing] =  False\n",
            "split = train, use 100/100 images\n",
            "split = train, number of images = 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "758bFCAr6rvz",
        "outputId": "c36aca3f-3bac-4fa0-c12b-2681269d7497"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I install the pre-trained model to improve on"
      ],
      "metadata": {
        "id": "qDB5yZodX9Qz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC2DEP4M0OsS",
        "outputId": "d6766ee0-ce6f-4fac-bf72-e26678134472"
      },
      "source": [
        "!bash ./scripts/download_pix2pix_model.sh sat2map"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: available models are edges2shoes, sat2map, map2sat, facades_label2photo, and day2night\n",
            "Specified [sat2map]\n",
            "WARNING: timestamping does nothing in combination with -O. See the manual\n",
            "for details.\n",
            "\n",
            "--2021-12-02 09:21:55--  http://efrosgans.eecs.berkeley.edu/pix2pix/models-pytorch/sat2map.pth\n",
            "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.244.190\n",
            "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.244.190|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 217704720 (208M)\n",
            "Saving to: ‘./checkpoints/sat2map_pretrained/latest_net_G.pth’\n",
            "\n",
            "./checkpoints/sat2m 100%[===================>] 207.62M  3.26MB/s    in 54s     \n",
            "\n",
            "2021-12-02 09:22:49 (3.83 MB/s) - ‘./checkpoints/sat2map_pretrained/latest_net_G.pth’ saved [217704720/217704720]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I finetune the model with the data pairs"
      ],
      "metadata": {
        "id": "WLX9ZJgfYDM8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sp7TCT2x9dB",
        "outputId": "ee24b100-3689-4366-de04-68da0c93a0f9"
      },
      "source": [
        "!python train.py --dataroot /content/drive/MyDrive/BayAreaHighway250AB_pix --name sat2map_pretrained --model pix2pix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- Options ---------------\n",
            "               batch_size: 1                             \n",
            "                    beta1: 0.5                           \n",
            "          checkpoints_dir: ./checkpoints                 \n",
            "           continue_train: False                         \n",
            "                crop_size: 256                           \n",
            "                 dataroot: /content/drive/MyDrive/BayAreaHighway250AB_pix\t[default: None]\n",
            "             dataset_mode: aligned                       \n",
            "                direction: AtoB                          \n",
            "              display_env: main                          \n",
            "             display_freq: 400                           \n",
            "               display_id: 1                             \n",
            "            display_ncols: 4                             \n",
            "             display_port: 8097                          \n",
            "           display_server: http://localhost              \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 1                             \n",
            "                 gan_mode: vanilla                       \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: True                          \t[default: None]\n",
            "                lambda_L1: 100.0                         \n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 286                           \n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: inf                           \n",
            "                    model: pix2pix                       \t[default: cycle_gan]\n",
            "                 n_epochs: 100                           \n",
            "           n_epochs_decay: 100                           \n",
            "               n_layers_D: 3                             \n",
            "                     name: sat2map_pretrained            \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: unet_256                      \n",
            "                      ngf: 64                            \n",
            "               no_dropout: False                         \n",
            "                  no_flip: False                         \n",
            "                  no_html: False                         \n",
            "                     norm: batch                         \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: train                         \n",
            "                pool_size: 0                             \n",
            "               preprocess: resize_and_crop               \n",
            "               print_freq: 100                           \n",
            "             save_by_iter: False                         \n",
            "          save_epoch_freq: 5                             \n",
            "         save_latest_freq: 5000                          \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "         update_html_freq: 1000                          \n",
            "                use_wandb: False                         \n",
            "                  verbose: False                         \n",
            "----------------- End -------------------\n",
            "dataset [AlignedDataset] was created\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "The number of training images = 100\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 33, in <module>\n",
            "    model = create_model(opt)      # create a model given opt.model and other options\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/__init__.py\", line 65, in create_model\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py\", line 57, in __init__\n",
            "    not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/networks.py\", line 156, in define_G\n",
            "    net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/networks.py\", line 460, in __init__\n",
            "    unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/networks.py\", line 497, in __init__\n",
            "    stride=2, padding=1, bias=use_bias)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 435, in __init__\n",
            "    False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 138, in __init__\n",
            "    self.reset_parameters()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 144, in reset_parameters\n",
            "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/init.py\", line 395, in kaiming_uniform_\n",
            "    return tensor.uniform_(-bound, bound)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UkcaFZiyASl"
      },
      "source": [
        "# Testing\n",
        "\n",
        "-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`\n",
        "\n",
        "Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.\n",
        "\n",
        "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
        "> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.\n",
        "\n",
        "> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).\n",
        "\n",
        "> See a list of currently available models at ./scripts/download_pix2pix_model.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I pair the test set to create the test set masks"
      ],
      "metadata": {
        "id": "KKaWQXSrYK-N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI2ulrpTFpWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2e655e-0795-4751-8ad0-b39b270556e8"
      },
      "source": [
        "!python datasets/combine_A_and_B.py --fold_A /content/drive/MyDrive/NOLATest --fold_B /content/drive/MyDrive/NOLATest --fold_AB /content/drive/MyDrive/NOLATestAB"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold_A] =  /content/drive/MyDrive/NOLATest\n",
            "[fold_B] =  /content/drive/MyDrive/NOLATest\n",
            "[fold_AB] =  /content/drive/MyDrive/NOLATestAB\n",
            "[num_imgs] =  1000000\n",
            "[use_AB] =  False\n",
            "[no_multiprocessing] =  False\n",
            "split = test, use 5/5 images\n",
            "split = test, number of images = 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I run inference on the test set. I would change this to run inference on 250 unpaired highway images to create the training masks for the \"full model\" configuration. Within utils I edit:     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def save_images(webpage, visuals, image_path, aspect_ratio=1.0, width=256, use_wandb=False):\n",
        "    \"\"\"Save images to the disk.\n",
        "\n",
        "    Parameters:\n",
        "        webpage (the HTML class) -- the HTML webpage class that stores these imaegs (see html.py for more details)\n",
        "        visuals (OrderedDict)    -- an ordered dictionary that stores (name, images (either tensor or numpy) ) pairs\n",
        "        image_path (str)         -- the string is used to create image paths\n",
        "        aspect_ratio (float)     -- the aspect ratio of saved images\n",
        "        width (int)              -- the images will be resized to width x width\n",
        "\n",
        "    This function will save images stored in 'visuals' to the HTML file specified by 'webpage'.\n",
        "    \"\"\"\n",
        "    image_dir = webpage.get_image_dir()\n",
        "    short_path = ntpath.basename(image_path[0])\n",
        "    name = os.path.splitext(short_path)[0]\n",
        "\n",
        "    webpage.add_header(name)\n",
        "    ims, txts, links = [], [], []\n",
        "    ims_dict = {}\n",
        "    for label, im_data in visuals.items():\n",
        "        im = util.tensor2im(im_data)\n",
        "        image_name = '%s_%s.png' % (name, label)\n",
        "        save_path = os.path.join(image_dir, image_name)\n",
        "        util.save_image(im, save_path, aspect_ratio=aspect_ratio)\n",
        "        ims.append(image_name)\n",
        "        txts.append(label)\n",
        "        links.append(image_name)\n",
        "        if use_wandb:\n",
        "            ims_dict[label] = wandb.Image(im)\n",
        "    webpage.add_images(ims, txts, links, width=width)\n",
        "    if use_wandb:\n",
        "        wandb.log(ims_dict)\n",
        "```\n",
        "\n",
        "With\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def save_images(webpage, visuals, image_path, aspect_ratio=1.0, width=256, use_wandb=False):\n",
        "    \"\"\"Save images to the disk.\n",
        "\n",
        "    Parameters:\n",
        "        webpage (the HTML class) -- the HTML webpage class that stores these imaegs (see html.py for more details)\n",
        "        visuals (OrderedDict)    -- an ordered dictionary that stores (name, images (either tensor or numpy) ) pairs\n",
        "        image_path (str)         -- the string is used to create image paths\n",
        "        aspect_ratio (float)     -- the aspect ratio of saved images\n",
        "        width (int)              -- the images will be resized to width x width\n",
        "\n",
        "    This function will save images stored in 'visuals' to the HTML file specified by 'webpage'.\n",
        "    \"\"\"\n",
        "    image_dir = webpage.get_image_dir()\n",
        "    short_path = ntpath.basename(image_path[0])\n",
        "    name = os.path.splitext(short_path)[0]\n",
        "\n",
        "    webpage.add_header(name)\n",
        "    ims, txts, links = [], [], []\n",
        "    ims_dict = {}\n",
        "    for label, im_data in visuals.items():\n",
        "      if label == \"fake_B\"\n",
        "        im = util.tensor2im(im_data)\n",
        "        image_name = '%s_%s.png' % (name, label)\n",
        "        save_path = os.path.join(image_dir, image_name)\n",
        "        util.save_image(im, save_path, aspect_ratio=aspect_ratio)\n",
        "        ims.append(image_name)\n",
        "        txts.append(label)\n",
        "        links.append(image_name)\n",
        "        if use_wandb:\n",
        "            ims_dict[label] = wandb.Image(im)\n",
        "    webpage.add_images(ims, txts, links, width=width)\n",
        "    if use_wandb:\n",
        "        wandb.log(ims_dict)\n",
        "```\n",
        "\n",
        "To create cleaner folders with only the test masks I need\n"
      ],
      "metadata": {
        "id": "g4fPEnxiYRfP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCsKkEq0yGh0",
        "outputId": "712e8cf8-e122-41c4-cf2e-746121bed025"
      },
      "source": [
        "!python test.py --dataroot /content/drive/MyDrive/NOLATestAB --model pix2pix --name sat2map_pretrained --checkpoints_dir /content/drive/MyDrive/checkpoints"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- Options ---------------\n",
            "             aspect_ratio: 1.0                           \n",
            "               batch_size: 1                             \n",
            "          checkpoints_dir: /content/drive/MyDrive/checkpoints\t[default: ./checkpoints]\n",
            "                crop_size: 256                           \n",
            "                 dataroot: /content/drive/MyDrive/NOLATestAB\t[default: None]\n",
            "             dataset_mode: aligned                       \n",
            "                direction: AtoB                          \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "                     eval: False                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: False                         \t[default: None]\n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 256                           \n",
            "         max_dataset_size: inf                           \n",
            "                    model: pix2pix                       \t[default: test]\n",
            "               n_layers_D: 3                             \n",
            "                     name: sat2map_pretrained            \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: unet_256                      \n",
            "                      ngf: 64                            \n",
            "               no_dropout: False                         \n",
            "                  no_flip: False                         \n",
            "                     norm: batch                         \n",
            "                 num_test: 50                            \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: test                          \n",
            "               preprocess: resize_and_crop               \n",
            "              results_dir: ./results/                    \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "                use_wandb: False                         \n",
            "                  verbose: False                         \n",
            "----------------- End -------------------\n",
            "dataset [AlignedDataset] was created\n",
            "initialize network with normal\n",
            "model [Pix2PixModel] was created\n",
            "loading the model from /content/drive/MyDrive/checkpoints/sat2map_pretrained/latest_net_G.pth\n",
            "---------- Networks initialized -------------\n",
            "[Network G] Total number of parameters : 54.414 M\n",
            "-----------------------------------------------\n",
            "creating web directory ./results/sat2map_pretrained/test_latest\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "processing (0000)-th image... ['/content/drive/MyDrive/NOLATestAB/test/output_1.png']\n"
          ]
        }
      ]
    }
  ]
}